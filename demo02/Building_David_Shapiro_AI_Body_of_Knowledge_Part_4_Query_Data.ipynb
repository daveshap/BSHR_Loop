{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tractorjuice/BSHR_Loop/blob/main/demo02/Building_David_Shapiro_AI_Body_of_Knowledge_Part_4_Query_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c1e3b9"
      },
      "source": [
        "# David Shapiro AI Body of Knowledge Using Langchain & OpenAI\n",
        "## Part 4, query the vector database using ChatGPT\n",
        "\n",
        "This example shows how to create and query an internal knowledge base using ChatGPT.\n",
        "\n",
        "This does not require a GPU runtime."
      ],
      "id": "07c1e3b9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "source": [
        "## Set Up\n"
      ],
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "JSzQ8Ud4WeiH"
      },
      "id": "JSzQ8Ud4WeiH"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8Uynu3LfWYst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdae9be-82bf-40e1-8599-778b62d42c3d"
      },
      "id": "8Uynu3LfWYst",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "KB_FOLDER = \"/content/gdrive/Shareddrives/AI/DavidShapiroKB\"  # Google drive folder to save the knowledgebase\n",
        "YT_DATASTORE = os.path.join(KB_FOLDER, \"youtube/datastore\")  # Sub-directory for YouTube FAIS datastore files\n",
        "YT_AUDIO_FOLDER = os.path.join(KB_FOLDER, \"youtube/audio\")  # Sub-directory for audio files\n",
        "TRANSCRIPTS_FOLDER = os.path.join(YT_AUDIO_FOLDER, \"transcripts\")  # Sub-directory for transcripts of audio files\n",
        "TRANSCRIPTS_TEXT_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"text\")  # Sub-directory for text of audio files\n",
        "TRANSCRIPTS_WHISPER_FOLDER = os.path.join(TRANSCRIPTS_FOLDER, \"whisper_chunks\")  # Sub-directory for Whisper chunks of audio files\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(KB_FOLDER):\n",
        "    os.makedirs(KB_FOLDER)\n",
        "\n",
        "# Check if directory exists and if not, create it\n",
        "if not os.path.exists(YT_DATASTORE):\n",
        "    os.makedirs(YT_DATASTORE)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(YT_AUDIO_FOLDER):\n",
        "    os.makedirs(YT_AUDIO_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_TEXT_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_TEXT_FOLDER)\n",
        "\n",
        "# Check if sub-directory exists and if not, create it\n",
        "if not os.path.exists(TRANSCRIPTS_WHISPER_FOLDER):\n",
        "    os.makedirs(TRANSCRIPTS_WHISPER_FOLDER)"
      ],
      "metadata": {
        "id": "vWeQuyzjU0m_"
      },
      "id": "vWeQuyzjU0m_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load required dependencies"
      ],
      "metadata": {
        "id": "C2dzEuzz2_di"
      },
      "id": "C2dzEuzz2_di"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6TkUqCJB_2X"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchainhub\n",
        "!pip install -q openai\n",
        "!pip install -q tiktoken"
      ],
      "id": "W6TkUqCJB_2X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03__ie72B8J_"
      },
      "source": [
        "Use Pinecone or FAISS for the Vector Database"
      ],
      "id": "03__ie72B8J_"
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = 'FAISS' # Set to 'Pinecone' or 'FAISS' for the vector datbase"
      ],
      "metadata": {
        "id": "cckWJ2E2TAjK"
      },
      "id": "cckWJ2E2TAjK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    !pip install -q pinecone-client\n",
        "    from langchain.vectorstores import Pinecone\n",
        "    from tqdm.autonotebook import tqdm\n",
        "    import pinecone\n",
        "\n",
        "    # initialize pinecone\n",
        "    pinecone.init(\n",
        "        api_key=\"\",  # find at app.pinecone.io\n",
        "        environment=\"us-west4-gcp-free\"  # next to api key in console\n",
        "        )\n",
        "\n",
        "    index_name = \"knowledge\" # Put your Pincecone index name here\n",
        "    name_space = \"DavidShapiroKB\" # Put your Pincecone namespace here\n",
        "\n",
        "else:\n",
        "    !pip install -q faiss-cpu\n",
        "    from langchain.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "YoCtnnwMS9V9"
      },
      "id": "YoCtnnwMS9V9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUbD-eXCXNy-"
      },
      "source": [
        "Set up OPEN_API_KEY and necessary variables"
      ],
      "id": "TUbD-eXCXNy-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY7CJZoh5cma"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-.....\" # Add you OpenAI Key here\n",
        "\n",
        "#MODEL = \"gpt-3\"\n",
        "#MODEL = \"gpt-3.5-turbo\"\n",
        "#MODEL = \"gpt-3.5-turbo-0613\"\n",
        "#MODEL = \"gpt-3.5-turbo-16k\"\n",
        "MODEL = \"gpt-3.5-turbo-16k-0613\"\n",
        "#MODEL = \"gpt-4\"\n",
        "#MODEL = \"gpt-4-0314\"\n",
        "#MODEL = \"gpt-4-0613\"\n",
        "#MODEL = \"gpt-4-32k-0613\""
      ],
      "id": "tY7CJZoh5cma"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "# Query using the vector store with ChatGPT integration"
      ],
      "id": "35f99145"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup access to the Pinecone or FAISS vector database"
      ],
      "metadata": {
        "id": "S5Vf831zQLJa"
      },
      "id": "S5Vf831zQLJa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibz6Tz_7iPeQ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "id": "ibz6Tz_7iPeQ"
    },
    {
      "cell_type": "code",
      "source": [
        "if vectorstore == 'Pinecone':\n",
        "    vector_store = Pinecone.from_existing_index(index_name, embeddings, namespace=name_space)\n",
        "\n",
        "else:\n",
        "    # Open datastore\n",
        "    from langchain.vectorstores import FAISS\n",
        "    if os.path.exists(f\"{YT_DATASTORE}\"):\n",
        "        vector_store = FAISS.load_local(\n",
        "            f\"{YT_DATASTORE}\",\n",
        "            OpenAIEmbeddings()\n",
        "            )\n",
        "    else:\n",
        "        print(f\"Missing files. Upload index.faiss and index.pkl files to data_store directory first\")\n"
      ],
      "metadata": {
        "id": "QQDKy5IRT98I"
      },
      "id": "QQDKy5IRT98I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the prompt"
      ],
      "metadata": {
        "id": "WDcuSm-wQPC-"
      },
      "id": "WDcuSm-wQPC-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "system_template=\"\"\"\n",
        "    You are an AI specialist bot.\n",
        "    You use examples from the provided YouTube videos in your answers.\n",
        "    Your language should be for an 12 year old to understand.\n",
        "    If you do not know the answer to a question, do not make information up - instead, ask a follow-up question in order to gain more context.\n",
        "    Use a mix of technical and colloquial uk english language to create an accessible and engaging tone.\n",
        "    Use the following pieces of context to answer the users question.\n",
        "    Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
        "----------------\n",
        "{summaries}\n",
        "\"\"\"\n",
        "messages = [\n",
        "    SystemMessagePromptTemplate.from_template(system_template),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "]\n",
        "prompt = ChatPromptTemplate.from_messages(messages)"
      ],
      "id": "32a49412"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise the LLM API"
      ],
      "metadata": {
        "id": "jlIfoZccQRmm"
      },
      "id": "jlIfoZccQRmm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3018f865"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "llm = ChatOpenAI(model_name=MODEL, temperature=0)  # Modify model_name if you have access to GPT-4\n",
        "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(), # Use MMR search and return 5 (max 20) video sources\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")"
      ],
      "id": "3018f865"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "source": [
        "#### Use the chain to query"
      ],
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the sources so we can find the YouTube videos"
      ],
      "metadata": {
        "id": "6_aZT0JXPr_9"
      },
      "id": "6_aZT0JXPr_9"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the BSHR Loop?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "dRFI-plXIlPY"
      },
      "id": "dRFI-plXIlPY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_video']}?t={int(document.metadata['start_time'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "metadata": {
        "id": "8b1IKUV4IuZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f66a993-8a5c-42c9-ce1e-11b3ae1f8860"
      },
      "id": "8b1IKUV4IuZQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the BSHR Loop?\n",
            "The BSHR Loop, or Basher Loop, is a method that humans use to find information or answers to their questions. It stands for Brainstorm, Search, Hunt, and Review. It is a process where you first brainstorm a list of search queries or keywords related to what you are looking for. Then, you use those keywords to search for information online. After that, you hunt through the search results to find the most relevant and useful information. Finally, you review the information you found to see if it answers your question or provides the information you need. So basically, the BSHR Loop is just a fancy way of describing the process that humans naturally go through when they are trying to find information.\n",
            "\n",
            "Source 1:\n",
            "Video title: GPT Prompt Strategy: Brainstorm, Search, Hypothesize, and Refine - THIS is the FUTURE!!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/sJLB2EIQUvQ?t=289\n",
            "Content: the Basher Loop, or BSHR.\n",
            "\n",
            "Source 2:\n",
            "Video title: GPT Prompt Strategy: Latent Space Activation - what EVERYONE is missing!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/N8p6u1OtARs?t=820\n",
            "Content: So the BSHR loop is basically just what humans do.\n",
            "\n",
            "Source 3:\n",
            "Video title: GPT Prompt Strategy: Brainstorm, Search, Hypothesize, and Refine - THIS is the FUTURE!!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/sJLB2EIQUvQ?t=290\n",
            "Content: So BSHR is the point of today's video,\n",
            "\n",
            "Source 4:\n",
            "Video title: GPT Prompt Strategy: Latent Space Activation - what EVERYONE is missing!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/N8p6u1OtARs?t=836\n",
            "Content: the BSHR loop is you brainstorm a list of search queries, which\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is David Shapiro in these videos?\"\n",
        "result = chain(query)"
      ],
      "metadata": {
        "id": "0bo2rD9cK2gm"
      },
      "id": "0bo2rD9cK2gm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcrUA39A86G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e8465c-50ed-46e3-a2be-5c2d18fe45d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is David Shapiro in these videos?\n",
            "David Shapiro is the person who created and is featured in these videos. He is the one speaking and providing the content in the videos.\n",
            "\n",
            "Source 1:\n",
            "Video title: First look at ChatGPT API - the age of Autonomous AI begins TODAY! Cognitive Architectures ahoy!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/si58om5_YBI?t=698\n",
            "Content: Username, David Shapiro,\n",
            "\n",
            "Source 2:\n",
            "Video title: Biological Immortality by 2030: Social & Economic Implications + Some Predictions!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/5ZImuCe6P2E?t=0\n",
            "Content: Hey everyone, David Shapiro here with a video.\n",
            "\n",
            "Source 3:\n",
            "Video title: The Burden of Competence: The Mixed Blessings of Talent & Giftedness\n",
            "Video author: Neurospicy with David Shapiro\n",
            "Source video: https://youtu.be/MSVDsgOKtyo?t=0\n",
            "Content: Hey everyone, David Shapiro here with a new video.\n",
            "\n",
            "Source 4:\n",
            "Video title: ChatGPT4 - Sparse Priming Representations, Hierarchical Memory Consolidation, and Implied Cognition!\n",
            "Video author: David Shapiro\n",
            "Source video: https://youtu.be/nDOmoIFx8Ww?t=0\n",
            "Content: Hey everybody, David Shapiro here with video.\n"
          ]
        }
      ],
      "source": [
        "print(result['question'])\n",
        "print(result['answer'])\n",
        "\n",
        "source_documents = result['source_documents']\n",
        "for index, document in enumerate(source_documents):\n",
        "    print(f\"\\nSource {index + 1}:\")\n",
        "    print(f\"Video title: {document.metadata['title']}\")\n",
        "    print(f\"Video author: {document.metadata['author']}\")\n",
        "    print(f\"Source video: https://youtu.be/{document.metadata['source_video']}?t={int(document.metadata['start_time'])}\")\n",
        "    print(f\"Content: {document.page_content}\")"
      ],
      "id": "AcrUA39A86G6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "collapsed_sections": [
        "0UqlAAxTXnGF",
        "S5Vf831zQLJa"
      ],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}